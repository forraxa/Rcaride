<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
      <title>
        Rcaride
      </title>
      <!-- Always force latest IE rendering engine (even in intranet) & Chrome Frame -->
      <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
      <meta content="width=device-width, initial-scale=1.0" name="viewport">
        <meta content="Rcaride apuntes" name="description"/>
        <meta content="R, Rstudio, spark, hadoop, mongodb, datawarehouse, pentaho, python, big data, machine learning" name="keywords"/>
        <meta content="Roberto Caride" name="author"/>
        <!-- <link rel="publisher" href="https://plus.google.com/117689250782136016574"> -->
        <!-- Le styles -->
        <link href="http://fonts.googleapis.com/css?family=Roboto:400,300,700" rel="stylesheet" type="text/css">
          <link href="assets/css/font-awesome.min.css" rel="stylesheet">
            <!--[if IE 7]>
      <link rel="stylesheet" href="assets/css/font-awesome-ie7.min.css">
      <![endif]-->
            <link href="assets/css/bootplus.css" rel="stylesheet">
              <link href="assets/css/bootplus-responsive.css" rel="stylesheet">
                <link href="assets/css/docs.css" rel="stylesheet">
                  <link href="assets/js/google-code-prettify/prettify.css" rel="stylesheet">
                    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
                    <!--[if lt IE 9]>
        <script src="assets/js/html5shiv.js"></script>
      <![endif]-->
                    <!-- Le fav and touch icons -->
                    <link href="assets/ico/apple-touch-icon-144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144">
                      <link href="assets/ico/apple-touch-icon-114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114">
                        <link href="assets/ico/apple-touch-icon-72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72">
                          <link href="assets/ico/apple-touch-icon-57-precomposed.png" rel="apple-touch-icon-precomposed">
                            <link href="assets/ico/favicon.png" rel="shortcut icon">
                              <script type="text/javascript">
                                var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-3182578-9']);
        _gaq.push(['_trackPageview']);

        (function() {
          var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
          ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
          var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
                              </script>
                            </link>
                          </link>
                        </link>
                      </link>
                    </link>
                  </link>
                </link>
              </link>
            </link>
          </link>
        </link>
      </meta>
    </meta>
  </head>
  <body data-spy="scroll" data-target=".bs-docs-sidebar">
    <!-- Navbar
    ================================================== -->
    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <button class="btn btn-navbar" data-target=".nav-collapse" data-toggle="collapse" type="button">
            <span class="icon-bar">
            </span>
            <span class="icon-bar">
            </span>
            <span class="icon-bar">
            </span>
          </button>
          <a class="brand" href="./index.html">
            Rcaride
          </a>
          <div class="nav-collapse collapse">
            <ul class="nav">
              <li class="active">
                <a href="./index.html">
                  Inicio
                </a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <!-- Subhead
================================================== -->
    <header class="jumbotron subhead masthead" id="overview">
      <div class="container">
        <h1>
          Apuntes, análisis y proyectos
        </h1>
        <p class="lead">
          R, spark, hadoop, mongodb, datawarehouse, pentaho, python, big data, machine learning...
        </p>
      </div>
    </header>
    <div class="container">
      <!-- Docs nav
    ================================================== -->
      <div class="row">
        <div class="span3 bs-docs-sidebar">
          <ul class="nav nav-list bs-docs-sidenav">
            <li>
              <a href="#fundamentos-head">
                1. Fundamentos
              </a>
            </li>
            <li>
              <a href="#instalacion-head">
                2. Instalación
              </a>
            </li>
            <li>
              <a href="#rdd-dataframe">
                3. RDD
              </a>
            </li>
            <li>
              <a href="#transformaciones">
                4. Transformaciones
              </a>
            </li>
            <li>
              <a href="#acciones">
                5. Acciones
              </a>
            </li>
            <li>
              <a href="#persistencia-head">
                6. Persistencia y acumuluadores
              </a>
            </li>
          </ul>
        </div>
        <div class="span9">
          <!-- Fundamentos
        ================================================== -->
          <section id="fundamentos-head">
            <div class="page-header">
              <h1>
                1. Fundamentos
              </h1>
            </div>
            <p class="lead">
              SPARK: Sistema de computación de datos.
            </p>
            <ul>
              <li>
                Se integra a la perfección con Hadoop
              </li>
              <li>
                Trabaja en memoria y disco aunque su máximo rendimiento lo obtiene en memoria, hasta 100 veces más rápido
              </li>
              <li>
                Tiene API Java, Python, Scala, R
              </li>
              <li>
                Spark permite el procesamiento en casi tiempo real (mini batch)
              </li>
              <li>
                <strong>
                  RDD:
                </strong>
                Resilient Distributed Dataset.
                <p>
                  Son los datos distribuidos en memoria
                </p>
              </li>
              <li>
                Usa evaluación perezosa.
                <p>
                  No ejecuta el código en las transformación, sólo en las acciones sobre los datos
                </p>
              </li>
            </ul>
            <hr>
              <p class="lead">
                Componentes principales.
              </p>
              <ul>
                <li>
                  Spark Core: Base donde se apoyan los demás componentes
                </li>
                <li>
                  Spark SQL: Procesamiento de datos estructurados y semi-estructurados
                </li>
                <li>
                  Spark Streaming: Procesamineto de datos en tiempo casi real
                </li>
                <li>
                  Spark MLlib (machine learning): Librería de Machine Learning
                </li>
                <li>
                  Spark GraphX: Procesamineto de grafos.
                  <p>
                    DAG: Grafo asíncrono dirigido
                  </p>
                </li>
              </ul>
              <img alt="" src="assets/img/spark-stack.png"/>
              <hr>
                <p class="lead">
                  Arquitectura de Spark
                </p>
                <img alt="" src="assets/img/arquitectura-spark.png"/>
                <hr>
                  <p class="lead">
                    SparkContext
                  </p>
                  <p>
                    Es el objeto que especifica como vamos a acceder a nuestro cluster
                  </p>
                  <p>
                    Es el contexto donde se crean las variables en Spark
                  </p>
                  <p>
                    Se instancia con la variable "sc" (no todos los entornos)
                  </p>
                  <p>
                    La creación de un objeto
                    <strong>
                      SparkContext
                    </strong>
                    lleva implicitamente un objeto
                    <strong>
                      SparkConf
                    </strong>
                  </p>
                  <hr>
                    <p class="lead">
                      SparkConf
                    </p>
                    <p>
                      Este objeto contien la información sobre nuestra aplicación.
                    </p>
                    <hr>
                      <p class="lead">
                        Cluster Manager - Gestores de recursos.
                      </p>
                      <ul>
                        <li>
                          <strong>
                            Standalone:
                          </strong>
                          Viene incluido en Spark (es muy sencillo)
                        </li>
                        <li>
                          <strong>
                            Mesos:
                          </strong>
                          Gestor de recursos de Spark, puede ejecutar Hadoop, Map Reduce y aplicaciones de servicio
                        </li>
                        <li>
                          <strong>
                            YARN:
                          </strong>
                          Gestor de recursos propio de Hadoop 2, puede ser utilizado por SPARK
                        </li>
                      </ul>
                      <hr>
                        <p class="lead">
                          Executors
                        </p>
                        <p>
                          Son los encargados de ejecutar las
                          <strong>
                            tareas o task
                          </strong>
                          en los nodos del cluster a petición del
                          <strong>
                            Cluster Manager
                          </strong>
                        </p>
                        <hr>
                          <p class="lead">
                            RDD Resilient Distributed Datasets
                          </p>
                          <p>
                            Conjunto de datos distribuidos en memoria donde la memoria está configurada a modo de un cluster por lo que el tratamiento de los datos se realiza de forma paralela, rápida y tolerante a fallos.
                          </p>
                          <h4>
                            Tipos de RDD según su origen:
                          </h4>
                          <ul>
                            <li>
                              <strong>
                                Colecciones paralelizadas:
                              </strong>
                              basadas en colecciones de Scala
                            </li>
                            <li>
                              <strong>
                                Datasets de Hadoop:
                              </strong>
                              creados a partir de ficheros almacenados en HDFS.
                            </li>
                          </ul>
                          <h4>
                            Tipos de operaciones sobre un RDD:
                          </h4>
                          <ul>
                            <li>
                              <strong>
                                Transformaciones:
                              </strong>
                              Crean nuevos conjutos de datos
                            </li>
                            <ul>
                              <li>
                                <strong>
                                  Narrow:
                                </strong>
                                Las transformaciones se han de realizar mezclando distintas particiones.
                                <p>
                                  filter(), sample(), map(), flatmap()
                                </p>
                              </li>
                              <li>
                                <strong>
                                  Wide:
                                </strong>
                                Las transformaciones se realizan en la su propia partición.
                                <p>
                                  groupByKey(), reduceByKey()
                                </p>
                              </li>
                            </ul>
                            <li>
                              <strong>
                                Acciones:
                              </strong>
                              Devuelven el valor al driver del cluster tras realizar un computo sobre los datos.
                              <p>
                                reduce(), collect(), count(), first(), take()
                              </p>
                            </li>
                          </ul>
                        </hr>
                      </hr>
                    </hr>
                  </hr>
                </hr>
              </hr>
            </hr>
          </section>
          <!-- Instalación
        ================================================== -->
          <section id="instalacion-head">
            <div class="page-header">
              <h1>
                2. Instalación
              </h1>
            </div>
            <p class="lead">
              3 modalidades de instalación.
            </p>
            <ul>
              <li>
                <strong>
                  Standalone:
                </strong>
                <p>
                  Spark + HDFS
                </p>
              </li>
              <li>
                <strong>
                  Hadoop V1:
                </strong>
                <p>
                  Spark + Map Reduce + HDFS
                </p>
              </li>
              <li>
                <strong>
                  Hadoop V2:
                </strong>
                <p>
                  Spark + YARN/Mesos + HDFS
                </p>
              </li>
            </ul>
            <hr>
              <p class="lead">
                Instalación de Standalone.
              </p>
              <p>
                Se instalará a partir de una Máquina Virtual de 64bits con Ubuntu 16.04.
              </p>
              <p>
                La máquina incluye:
              </p>
              <ul>
                <li>
                  Apache Spark 2.2
                </li>
                <li>
                  Python 3.5.2
                </li>
                <li>
                  Jupyter Notebook
                </li>
                <li>
                  Notebook Spark con Kernel para Python 3.5, Scala (SPylon) y R (IRKernel)
                </li>
              </ul>
              <p>
                Para instalar la VM se necesita:
              </p>
              <ul>
                <li>
                  <a href="https://www.virtualbox.org/">
                    Virtualbox
                  </a>
                  5.0 o superior.
                </li>
                <li>
                  <a href="https://www.vagrantup.com/">
                    Vagrant
                  </a>
                  1.8 o superior.
                </li>
              </ul>
              <hr>
                <p class="lead">
                  Proceso de instalación.
                </p>
                <ol>
                  <li>
                    <p>
                      Clonar o descargar la máquina Virtual,
                      <a href="https://github.com/paulovn/ml-vm-notebook">
                        Descargar
                      </a>
                    </p>
                  </li>
                  <li>
                    Abrir un terminal y situarnos dentro de la carpeta donde hayamos descomprimido la máquina virtual, dentro de la carpeta encontraremos el fichero
                    <strong>
                      Vagrantfile
                    </strong>
                  </li>
                  <li>
                    Ejecutamos
                    <strong>
                      vagrant up
                    </strong>
                    para levantar la máquina
                    <p>
                      Esto preparará la máquina virtual y lanzará el notebook de Jupyter en el puerto 8008.
                    </p>
                    <p>
                      La contraseña para entrar es "
                      <strong>
                        vmuser
                      </strong>
                      "
                    </p>
                  </li>
                </ol>
                <p>
                  <strong>
                    vagrant halt
                  </strong>
                  pausa la VM
                </p>
                <p>
                  <strong>
                    vagrant destroy
                  </strong>
                  Elimina la instalación de la VM
                </p>
                <p>
                  Editar
                  <strong>
                    vagrantfile
                  </strong>
                  para modificar la configuración de la VM
                </p>
                <p>
                  <strong>
                    vagrant ssh
                  </strong>
                  para acceso a un terminal de la VM
                </p>
                <hr>
                  <p>
                    <strong>
                      http://localhost:8008
                    </strong>
                    para acceder al notebook
                  </p>
                  <p>
                    <strong>
                      http://localhost:4040/
                    </strong>
                    para monitorizar los jobs lanzados en Spark
                  </p>
                </hr>
              </hr>
            </hr>
          </section>
          <!-- RDD, Dataframe y Dataset
        ================================================== -->
          <section id="rdd-dataframe">
            <div class="page-header">
              <h1>
                3. RDD
              </h1>
            </div>
            <p>
              Los RDD son los datos que se han leido y se han particionado en los nodos del cluster que se crea en la memoria
            </p>
            <p>
              Los orígenes de los datos pueden ser diversos pero una fuente de datos muy común es HDFS
            </p>
            <p>
              Contra los RDD se pueden lanzar:
            </p>
            <ul>
              <li>
                <strong>
                  Transformaciones:
                </strong>
                Crea un nuevo RDD a partir de otro existente
              </li>
              <li>
                <strong>
                  Acciones:
                </strong>
                Genera un valor que es mandado al
                <strong>
                  driver
                </strong>
              </li>
            </ul>
            <div class="row">
              <div class="span4">
                <div class="row">
                  <p style="margin-left:30px"><strong>Transformaciones:</strong></p>
                  <div class="span1">
                    <ul>
                     <li>map</li>
                     <li>filter</li>
                     <li>flatMap</li>
                     <li>union</li>
                     <li>intersection</li>
                     <li>distinct</li>
                   </ul>
                  </div>
                  <div class="span1">
                    <ul>
                     <li>groupByKey</li>
                     <li>reduceByKey</li>
                     <li>sortByKey</li>
                     <li>join</li>
                     <li>cogroup</li>
                     <li>coalesce</li>
                   </ul>
                  </div>
                </div>
              </div>



              <div class="span4">
                <div class="row">
                  <p><strong>Acciones:</strong></p>
                  <div class="span1">
                    <ul>
                     <li>reduce</li>
                     <li>collect</li>
                     <li>count</li>
                     <li>first</li>
                     <li>take</li>
                   </ul>
                  </div>
                  <div class="span1">
                    <ul>
                     <li>saveAsTextFile</li>
                     <li>max, min...</li>
                     <li>countByKey</li>
                     <li>foreach</li>
                   </ul>
                  </div>
                </div>
              </div>
              






            </div>
            <hr>
              <p>
                Creación de RDDs:
              </p>
              <div class="codigo-scala">
                <div class="border-box-sizing" id="notebook" tabindex="-1">
                  <div id="notebook-container">
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [1]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//importamos SparkConf, SparkContext, RDD</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span> 
<span class="k">import</span> <span class="nn">org.apache.spark.SparkContext</span>
<span class="k">import</span> <span class="nn">org.apache.spark.rdd.RDD</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt">
                            </div>
                            <div class="output_text output_subarea ">
                              <pre>Intitializing Scala interpreter ...</pre>
                            </div>
                          </div>
                          <div class="output_area">
                            <div class="prompt">
                            </div>
                            <div class="output_text output_subarea ">
                              <pre>Spark Web UI available at http://10.0.2.15:4040
SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1531827357926)
SparkSession available as 'spark'
</pre>
                            </div>
                          </div>
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[1]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [2]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//comprobamos que se ha creado el objeto SparkContext</span>
<span class="n">sc</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[2]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@66265b45
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [3]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//se obtiene la configuración actual de SparkConf</span>
<span class="n">sc</span><span class="o">.</span><span class="n">getConf</span><span class="o">.</span><span class="n">getAll</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[3]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>res1: Array[(String, String)] = Array((spark.eventLog.enabled,true), (spark.app.id,local-1531827357926), (spark.repl.class.outputDir,/tmp/tmphl2gt916), (spark.repl.class.uri,spark://10.0.2.15:44097/classes), (spark.executor.id,driver), (spark.driver.host,10.0.2.15), (spark.app.name,spylon-kernel), (spark.driver.port,44097), (spark.rdd.compress,True), (spark.eventLog.dir,/var/log/ipnb), (spark.serializer.objectStreamReset,100), (spark.master,local[*]), (spark.submit.deployMode,client), (spark.ui.showConsoleProgress,true))
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [4]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//creación de un array</span>
<span class="k">val</span> <span class="n">cadenas</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">"spark"</span><span class="o">,</span> <span class="s">"hadoop"</span><span class="o">,</span> <span class="s">"hdfs"</span><span class="o">,</span> <span class="s">"spark"</span><span class="o">,</span> <span class="s">"hdfs"</span><span class="o">)</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[4]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>cadenas: Array[String] = Array(spark, hadoop, hdfs, spark, hdfs)
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [5]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//creación del primer RDD mediante parallelize. </span>
<span class="c1">//indicar cantidad de nodos para paralelizar los datos.</span>
<span class="k">val</span> <span class="n">cadenasRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">cadenas</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[5]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>cadenasRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:32
</console></pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [6]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//Petición de contenido con collect()</span>
<span class="c1">//Especial cuidad de llamar a collect() con RDD muy grandes.</span>
<span class="n">cadenasRDD</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[6]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>res2: Array[String] = Array(spark, hadoop, hdfs, spark, hdfs)
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [7]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//declaración de RDD junto con Array.</span>
<span class="k">val</span> <span class="n">numerosRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">,</span><span class="mi">6</span><span class="o">,</span><span class="mi">7</span><span class="o">,</span><span class="mi">8</span><span class="o">,</span><span class="mi">9</span><span class="o">),</span> <span class="mi">3</span><span class="o">)</span>
<span class="n">numerosRDD</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[7]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>numerosRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:29
res3: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)
</console></pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [8]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//crear un RDD a partir de leer fichero desde windows</span>
<span class="k">val</span> <span class="n">file</span> <span class="k">=</span><span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">"./documentos/texto.txt"</span><span class="o">)</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[8]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>file: org.apache.spark.rdd.RDD[String] = ./documentos/texto.txt MapPartitionsRDD[3] at textFile at <console>:29
</console></pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [9]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="n">file</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[9]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>res4: Array[String] = Array(Esto es una prueba de un documento de texto para analizar con Spark., El perro de San Roque no tiene rabo porque Ramon Ramirez se lo ha cortado.)
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [10]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//filtrado de datos, filtra la línea que contenga la palabra Spark</span>
<span class="k">val</span> <span class="n">filtro</span> <span class="k">=</span> <span class="n">file</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">line</span> <span class="k">=></span> <span class="n">line</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="s">"perro"</span><span class="o">))</span>
<span class="n">filtro</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[10]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>filtro: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:31
res5: Array[String] = Array(El perro de San Roque no tiene rabo porque Ramon Ramirez se lo ha cortado.)
</console></pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="cell border-box-sizing code_cell rendered">
                      <div class="input">
                        <div class="prompt input_prompt">
                          In [11]:
                        </div>
                        <div class="inner_cell">
                          <div class="input_area">
                            <div class=" highlight hl-scala">
                              <pre><span></span><span class="c1">//devolver sólo el primer elemento</span>
<span class="n">file</span><span class="o">.</span><span class="n">first</span><span class="o">()</span>
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                      <div class="output_wrapper">
                        <div class="output">
                          <div class="output_area">
                            <div class="prompt output_prompt">
                              Out[11]:
                            </div>
                            <div class="output_text output_subarea output_execute_result">
                              <pre>res6: String = Esto es una prueba de un documento de texto para analizar con Spark.
</pre>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </hr>
          </section>
          <!-- Transformaciones
        ================================================== -->
          <section id="transformaciones">
            <div class="page-header">
              <h1>
                4. Transformaciones
              </h1>
            </div>
            <div class="codigo-scala">
              <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//importamos SparkConf, SparkContext, RDD</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span> 
<span class="k">import</span> <span class="nn">org.apache.spark.SparkContext</span>
<span class="k">import</span> <span class="nn">org.apache.spark.rdd.RDD</span>

<span class="c1">//creación de un array</span>
<span class="k">val</span> <span class="n">cadenas</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">&quot;spark&quot;</span><span class="o">,</span> <span class="s">&quot;hadoop&quot;</span><span class="o">,</span> <span class="s">&quot;hdfs&quot;</span><span class="o">,</span> <span class="s">&quot;spark&quot;</span><span class="o">,</span> <span class="s">&quot;hdfs&quot;</span><span class="o">)</span>

<span class="c1">//paralelización del RDD cadenas</span>
<span class="k">val</span> <span class="n">cadenasRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">cadenas</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Intitializing Scala interpreter ...</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Spark Web UI available at http://10.0.2.15:4040
SparkContext available as &#39;sc&#39; (version = 2.3.0, master = local[*], app id = local-1531853649819)
SparkSession available as &#39;spark&#39;
</pre>
</div>

</div>

<div class="output_area">

<div class="prompt output_prompt">Out[1]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
cadenas: Array[String] = Array(spark, hadoop, hdfs, spark, hdfs)
cadenasRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:34
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>Funciones de transformación.<br>
map():</strong> Aplica una función a cada elemento del RDD.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//convertir a mayúsculas cada palabra</span>
<span class="c1">//la función se leería: Para cada palabra aplicar función toUpperCase()</span>
<span class="k">val</span> <span class="n">cadenasMayusculas</span> <span class="k">=</span> <span class="n">cadenasRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">palabra</span> <span class="k">=&gt;</span> <span class="n">palabra</span><span class="o">.</span><span class="n">toUpperCase</span><span class="o">())</span>
<span class="n">cadenasMayusculas</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[2]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>cadenasMayusculas: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at map at &lt;console&gt;:33
res0: Array[String] = Array(SPARK, HADOOP, HDFS, SPARK, HDFS)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>filter():</strong> Filtra cada palabra igual a la cadena.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="n">cadenasMayusculas</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="n">p</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="s">&quot;ARK&quot;</span><span class="o">))</span>
<span class="n">data</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[3]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at &lt;console&gt;:33
res1: Array[String] = Array(SPARK, SPARK)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>flatMap():</strong> Aplica una función a cada elemento y devuelve una única lista.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">mayusculas</span> <span class="k">=</span> <span class="n">cadenasRDD</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="n">p</span><span class="o">.</span><span class="n">toUpperCase</span><span class="o">())</span>
<span class="n">mayusculas</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[4]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>mayusculas: org.apache.spark.rdd.RDD[Char] = MapPartitionsRDD[3] at flatMap at &lt;console&gt;:31
res2: Array[Char] = Array(S, P, A, R, K, H, A, D, O, O, P, H, D, F, S, S, P, A, R, K, H, D, F, S)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">mayusLength</span> <span class="k">=</span> <span class="n">cadenasRDD</span><span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="nc">List</span><span class="o">(</span><span class="n">p</span><span class="o">.</span><span class="n">toUpperCase</span><span class="o">(),</span> <span class="n">p</span><span class="o">.</span><span class="n">length</span><span class="o">))</span>
<span class="n">mayusLength</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[5]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>mayusLength: org.apache.spark.rdd.RDD[Any] = MapPartitionsRDD[4] at flatMap at &lt;console&gt;:31
res3: Array[Any] = Array(SPARK, 5, HADOOP, 6, HDFS, 4, SPARK, 5, HDFS, 4)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>union():</strong> une RDDs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="k">val</span> <span class="n">cadenasBoth</span> <span class="k">=</span> <span class="n">cadenasRDD</span><span class="o">.</span><span class="n">union</span><span class="o">(</span><span class="n">cadenasMayusculas</span><span class="o">)</span>
<span class="n">cadenasBoth</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[6]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>cadenasBoth: org.apache.spark.rdd.RDD[String] = UnionRDD[5] at union at &lt;console&gt;:33
res4: Array[String] = Array(spark, hadoop, hdfs, spark, hdfs, SPARK, HADOOP, HDFS, SPARK, HDFS)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>distinct():</strong> elimina valores repetidos.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">cadenasBoth</span><span class="o">.</span><span class="n">distinct</span><span class="o">().</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[7]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res5: Array[String] = Array(hadoop, SPARK, hdfs, spark, HDFS, HADOOP)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>groupByKey():</strong> Agrupa por cada key única los valor.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//se crea un RDD de pares clave valor.</span>
<span class="k">val</span> <span class="n">parCV</span> <span class="k">=</span> <span class="n">cadenasRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">p</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
<span class="n">parCV</span><span class="o">.</span><span class="n">collect</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[8]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>parCV: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[9] at map at &lt;console&gt;:32
res6: Array[(String, Int)] = Array((spark,1), (hadoop,1), (hdfs,1), (spark,1), (hdfs,1))
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//se aplica groupByKey donde se realiza un agregado de claves por cada valor único </span>
<span class="k">val</span> <span class="n">group</span> <span class="k">=</span> <span class="n">parCV</span><span class="o">.</span><span class="n">groupByKey</span><span class="o">()</span>
<span class="n">group</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[9]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>group: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[10] at groupByKey at &lt;console&gt;:34
res7: Array[(String, Iterable[Int])] = Array((hadoop,CompactBuffer(1)), (hdfs,CompactBuffer(1, 1)), (spark,CompactBuffer(1, 1)))
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>reduceByKey():</strong> Realiza una agrupación por clave única y reduce los valores de cada clave.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//(_+_) indica sumar todos los valores</span>
<span class="c1">//(_+_) se puede expresar también como ((a,b) =&gt; a+b)</span>
<span class="k">val</span> <span class="n">suma</span> <span class="k">=</span> <span class="n">parCV</span><span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="k">_</span> <span class="o">+</span> <span class="k">_</span><span class="o">)</span>
<span class="n">suma</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[10]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>suma: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[11] at reduceByKey at &lt;console&gt;:35
res8: Array[(String, Int)] = Array((hadoop,1), (hdfs,2), (spark,2))
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>sortByKey():</strong> Ordena por clave.
true: orden ascendente
false: orden descendente</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">parCV</span><span class="o">.</span><span class="n">sortByKey</span><span class="o">(</span><span class="kc">false</span><span class="o">).</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[11]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res9: Array[(String, Int)] = Array((spark,1), (spark,1), (hdfs,1), (hdfs,1), (hadoop,1))
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>crear una función para pasarla a un map:</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//se crea la función con parámetro de entrada string y devuelve un string y un entero.</span>
<span class="k">def</span> <span class="n">tamanyo</span><span class="o">(</span><span class="n">s</span> <span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">:</span> <span class="o">(</span><span class="kt">String</span><span class="o">,</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">return</span><span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="n">s</span><span class="o">.</span><span class="n">length</span><span class="o">());</span>
<span class="o">}</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[12]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>tamanyo: (s: String)(String, Int)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">cadenasRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">tamanyo</span><span class="o">(</span><span class="k">_</span><span class="o">)).</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[13]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res10: Array[(String, Int)] = Array((spark,5), (hadoop,6), (hdfs,4), (spark,5), (hdfs,4))
</pre>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>
            </div>
          </section>
           <section id="acciones">
            <div class="page-header">
              <h1>
                5. Acciones
              </h1>
            </div>
            <div class="codigo-scala">
              <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div id="notebook-container">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//importamos SparkConf, SparkContext, RDD</span>
<span class="k">import</span> <span class="nn">org.apache.spark.SparkConf</span> 
<span class="k">import</span> <span class="nn">org.apache.spark.SparkContext</span>
<span class="k">import</span> <span class="nn">org.apache.spark.rdd.RDD</span>

<span class="c1">//creación de un array</span>
<span class="k">val</span> <span class="n">cadenas</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="s">&quot;spark&quot;</span><span class="o">,</span> <span class="s">&quot;hadoop&quot;</span><span class="o">,</span> <span class="s">&quot;hdfs&quot;</span><span class="o">,</span> <span class="s">&quot;spark&quot;</span><span class="o">,</span> <span class="s">&quot;hdfs&quot;</span><span class="o">)</span>

<span class="c1">//paralelización del RDD cadenas</span>
<span class="k">val</span> <span class="n">cadenasRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="n">cadenas</span><span class="o">,</span> <span class="mi">3</span><span class="o">)</span>

<span class="c1">//declaración y paralelización de RDD</span>
<span class="k">val</span> <span class="n">numerosRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="o">,</span><span class="mi">3</span><span class="o">,</span><span class="mi">4</span><span class="o">,</span><span class="mi">5</span><span class="o">,</span><span class="mi">6</span><span class="o">,</span><span class="mi">7</span><span class="o">,</span><span class="mi">8</span><span class="o">,</span><span class="mi">9</span><span class="o">),</span> <span class="mi">3</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Intitializing Scala interpreter ...</pre>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Spark Web UI available at http://10.0.2.15:4041
SparkContext available as &#39;sc&#39; (version = 2.3.0, master = local[*], app id = local-1531858672993)
SparkSession available as &#39;spark&#39;
</pre>
</div>

</div>

<div class="output_area">

<div class="prompt output_prompt">Out[1]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
cadenas: Array[String] = Array(spark, hadoop, hdfs, spark, hdfs)
cadenasRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:34
numerosRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:37
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>colect()</strong> muestra el contenido de un RDD contenido</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">cadenasRDD</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[2]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res0: Array[String] = Array(spark, hadoop, hdfs, spark, hdfs)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>first()</strong> devuelve el primer elemento de un RDD</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">cadenasRDD</span><span class="o">.</span><span class="n">first</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[3]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res1: String = spark
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>count()</strong> devuelve el número de elemento de un RDD</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">cadenasRDD</span><span class="o">.</span><span class="n">count</span><span class="o">()</span> <span class="o">+</span> <span class="n">numerosRDD</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[4]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res2: Long = 14
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>reduce()</strong> Aplica una acción a todos los elementos del RDD y devuelve un único elemento</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//aplicar reduce con la suma de los elementos contenidos en numerosRDD</span>
<span class="n">numerosRDD</span><span class="o">.</span><span class="n">reduce</span><span class="o">(</span><span class="k">_</span><span class="o">+</span><span class="k">_</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[5]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res3: Int = 45
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>take()</strong> devuelve n primeros elementos del RDD</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">numerosRDD</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[6]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res4: Array[Int] = Array(1, 2, 3)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>max()</strong> devuelve el número máximo de entre los elementos del RDD
<strong>min()</strong> devuelve el número mínimo de entre los elementos del RDD</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">numerosRDD</span><span class="o">.</span><span class="n">max</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[7]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res5: Int = 9
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>countByKey()</strong> devuelve el conteo de valores para cada única clave</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="c1">//se crea un RDD de pares clave valor.</span>
<span class="k">val</span> <span class="n">parCV</span> <span class="k">=</span> <span class="n">cadenasRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">p</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>
<span class="n">parCV</span><span class="o">.</span><span class="n">collect</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[8]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>parCV: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[2] at map at &lt;console&gt;:32
res6: Array[(String, Int)] = Array((spark,1), (hadoop,1), (hdfs,1), (spark,1), (hdfs,1))
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">parCV</span><span class="o">.</span><span class="n">countByKey</span><span class="o">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt output_prompt">Out[9]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>res7: scala.collection.Map[String,Long] = Map(hadoop -&gt; 1, hdfs -&gt; 2, spark -&gt; 2)
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<p><strong>foreach()</strong> aplicar una acción por cada elemento del RDD</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-scala"><pre><span></span><span class="n">cadenasRDD</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">p</span> <span class="k">=&gt;</span> <span class="n">println</span><span class="o">(</span><span class="s">&quot;La palabra es &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>La palabra es hadoop
La palabra es hdfs
La palabra es spark
La palabra es spark
La palabra es hdfs
</pre>
</div>
</div>

</div>
</div>

</div>
    </div>
  </div>  
            </div>
            </section>
             <section id="persistencia-head">
            <div class="page-header">
              <h1>
                6. Persistencia y acumuladores
              </h1>
            </div> 
            <p>Spark tiene evaluación perezosa.</p>
            <p>Puede ser necesario utilizar un RDD varias veces y para evitar el coste de tener que recalcular continuamente el RDD se utiliza la <strong>persistencia.</strong></p>
            <p>Cuando se persiste un dato cada nodo es el responsable de almacenar internamente su parte.</p>
            <p>Si un nodo con datos persistentes falla, Spark recalcula las particiones perdidas cuando sea necesario.</p>
            <p>Existen varios niveles de persistencia pudiendo especificar que Spark <strong>serialice o no los datos en disco, memoria o una combinación de ambas.</strong></p>
            <ul><strong>Tipos de persistencia</strong>
              <li><strong>MEMORY_ONLY</strong> : Sólo memoria</li>
              <li><strong>MEMORY_AND_DISK</strong> : Memoria y disco</li>
              <li><strong>MEMORY_AND_DISK_SER</strong> : Memoria y disco serializado</li>
              <li><strong>DISK_ONLY</strong> : Sólo disco</li>
              <li><strong>MEMORY_ONLY_SER</strong> : Sólo memoria serializada</li>
              <li><strong>MEMORY_ONLY_2</strong> : Sólo memoria (versión mejorada)</li>
            </ul>
            <p>Si la persistencia de datos excede la memoria disponible Spark despreciará automáticamente las particiones más antíguas.</p>
            </section>

        </div>
      </div>
    </div>




  


    <!-- Footer
    ================================================== -->
    <footer class="footer">
      <div class="container">
        <p>
          Roberto Caride,
          <a href="mailto:forraxa@gmail.com">
            forraxa@gmail.com
          </a>
        </p>
        <p>
          <a href="https://www.youtube.com/channel/UCI1EBKWrqdVqcGMlbLVSYjA?view_as=subscriber" target="_blank">
            Canal youtube
          </a>
        </p>
      </div>
    </footer>
    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="http://platform.twitter.com/widgets.js" type="text/javascript">
    </script>
    <script src="assets/js/jquery.js">
    </script>
    <script src="assets/js/bootstrap-transition.js">
    </script>
    <script src="assets/js/bootstrap-alert.js">
    </script>
    <script src="assets/js/bootstrap-modal.js">
    </script>
    <script src="assets/js/bootstrap-dropdown.js">
    </script>
    <script src="assets/js/bootstrap-scrollspy.js">
    </script>
    <script src="assets/js/bootstrap-tab.js">
    </script>
    <script src="assets/js/bootstrap-tooltip.js">
    </script>
    <script src="assets/js/bootstrap-popover.js">
    </script>
    <script src="assets/js/bootstrap-button.js">
    </script>
    <script src="assets/js/bootstrap-collapse.js">
    </script>
    <script src="assets/js/bootstrap-carousel.js">
    </script>
    <script src="assets/js/bootstrap-typeahead.js">
    </script>
    <script src="assets/js/bootstrap-affix.js">
    </script>
    <script src="assets/js/holder/holder.js">
    </script>
    <script src="assets/js/google-code-prettify/prettify.js">
    </script>
    <script src="assets/js/application.js">
    </script>
    <!-- Analytics
    ================================================== -->
    <script>
      var _gauges = _gauges || [];
      (function() {
        var t   = document.createElement('script');
        t.type  = 'text/javascript';
        t.async = true;
        t.id    = 'gauges-tracker';
        t.setAttribute('data-site-id', '4f0dc9fef5a1f55508000013');
        t.src = '//secure.gaug.es/track.js';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(t, s);
      })();
    </script>
  </body>
</html>
